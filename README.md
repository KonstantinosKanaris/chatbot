# Character-based Text Generation with PyTorch

## Table of Contents
* [Overview](#Overview)
* [Key Features](#Key--Features)
* [Data](#Data)
* [Development](#Development)
* [Execute](#Execute)
* [Experiment Tracking](#Experiment--Tracking)
* [References](#References)

## Overview ğŸ”
A chatbot framework with PyTorch for training and evaluating seq-to-seq models.

Implements 5 high-level processes:
- Downloads data, i.e., cornell-dialogs from a source url
- Parses query-response sequence pairs from the raw data
- Processes the query-response sequence pairs (normalizing, filtering, etc.)
- Creates training/evaluation assets, i.e., train_set, val_set, test_set
- Trains and evaluates a sequence-to-sequence PyTorch model consisting of encoder and decoder layers
- Chats with the trained bot

The training and evaluation processes are fully configurable through dedicated configuration
files. For instance, for the training process, users can define training hyperparameters, such
as number of epochs, batch size, model/optimizer settings, in a YAML file. This allows for
easy customization and experimentation with different configurations without modifying
the source code.

### Project Structure ğŸŒ²
```
chatbot
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml             | Pre-commit hooks
â”œâ”€â”€ Makefile                            | Development commands for code formatting, linting, etc
â”œâ”€â”€ Pipfile                             | Project's dependencies and their versions using pipenv format
â”œâ”€â”€ Pipfile.lock                        | Auto-generated file that locks the dependencies to specific versions for reproducibility
â”œâ”€â”€ README.md
â”œâ”€â”€ chatbot                             | The main Python package containing the project's source code
â”‚Â Â  â”œâ”€â”€ __about__.py              | Metadata about the project, i.e., version number, author information, etc
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __main__.py               | The entry point for running the package as a script. Calls one of the controllers
â”‚Â Â  â”œâ”€â”€ controllers.py            | Contains the training controller
â”‚Â Â  â”œâ”€â”€ engine
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ evaluator.py    | Contains the evaluation process
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ trainer.py      | Contains the training process
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.py        | Auxilliary functions/classes for training and evaluation such as EarlyStopping
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ attention.py    | Luong Attention layer for the seq2seq model
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ decoders.py     | Decoder with Luong Attention and decoders/samplers for inference
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ embeddings.py   | Creates embedding layer with or without pretrained embeddings
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ encoders.py     | Encoder for the seq2seq model
â”‚Â Â  â”‚Â Â  â””â”€â”€ loss.py         | Implementation of custom loss methods, i.e., MaskedNLLLoss
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ aux.py                | Auxilliary functions/classes used across the project
â”‚Â Â      â””â”€â”€ data
â”‚Â Â          â”œâ”€â”€ __init__.py
â”‚Â Â          â”œâ”€â”€ datasets.py       | Cornell Dialogs Dataset
â”‚Â Â          â”œâ”€â”€ vectorizer.py     | Vectorizes text sequences to numbers. Token-based vectorization
â”‚Â Â          â””â”€â”€ vocabulary.py     | Maps tokens to indices and vice versa

â”œâ”€â”€ checkpoints                         | Checkpoints directory
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ colours.mk                          | A Makefile fragment containing color codes for terminal output styling
â”œâ”€â”€ configs
â”‚Â Â  â”œâ”€â”€ evaluation.yaml           | Configuration parameters for the evaluation process
â”‚Â Â  â”œâ”€â”€ experiments.yaml          | Configuration parameters for the training process
â”‚Â Â  â””â”€â”€ logging.ini               | Configuration file for Python's logging module
â”œâ”€â”€ data_preparation.ipynb              | Cornel dialogs data downloading and preparation
â”œâ”€â”€ docs                                | Sphinx generated documentation
â”‚Â Â  â”œâ”€â”€build
â”‚Â Â  â”œâ”€â”€source
â”‚Â Â  â””â”€â”€ ...
â””â”€â”€ mypy.ini                            | Configuration file for the MyPy static type checker
```

## Key Features ğŸ”‘

### General
* **Customizable Experiments**: Easy configuration of training experiments by defining the necessary
training parameters in a yaml file.
* **Experiment Tracking**: Use of MLFlow for tracking training experiments
* **Available Documentation**: [Documentation](docs/build/html/modules.html)

### Training Process
* **Custom TQDM Bar**: ```Training: [11/50] | [155/156]:   22%|â–ˆâ–ˆâ–ˆâ–       , loss=2.44 [01:40<21:29]```
* **Checkpointing**: Option to resume training process from the last saved checkpoint
* **EarlyStopping**: Training process stops when the model's performance stops improving on the validation set
* **LRReduction**: Learning rate reduction during training (Not implemented yet)

### Seq2Seq Model
* **Encoder-Decoder Architecture**: Encoder and decoder layers are trained with separate weights, optimizers and
lr schedulers
* **Embedding Layer**: Can be initialized with pre-trained word embeddings, i.e. GLoVe embeddings
(Implementation exists)
* **Luong Attention**: Implementation of a decoder with global Luong attention based on the paper:
https://arxiv.org/abs/1508.04025
* **Scheduled Sampling**: Implementation of Scheduled Sampling with custom Decay Schedulers based on the paper:
https://arxiv.org/abs/1506.03099

### Evaluation
* **Various Search Decoders**: Greedy Search Decoder, Random (multinomial) Search Decoder,
(Beam Search Decoder-> TODO)
* **ChatBot UI**: Not implemented yet

## Data ğŸ“„

## Development ğŸ
Clone the repository:
  ```bash
  $ git clone https://github.com/KonstantinosKanaris/chatbot.git
  ```

### Set up the environment

#### Create environment
Python 3.10 is required.

- Create the environment and install the dependencies:
    ```bash
    $ pipenv --python 3.10
    $ pipenv install --dev
    ```
- Enable the newly-created virtual environment, with:
    ```bash
    $ pipenv shell
    ```

## Execute ğŸš€
All the commands should be executed from the project's root directory.

### Get data
> Download the cornell-dialogs data by executing:
>```bash
> $ python -m chatbot download --name cornell_dialogs --directory <path_to_a_local_directory>
>```

### Parse data
> Parse the cornell-dialogs data by executing:
>```bash
> $ python -m chatbot parse --filepath <path_to_cornell_dialogs_utterances> --save_path <filepath_to_save_parsed_txt_file>
>```

### Process data
> Process the cornell-dialogs data by executing:
>```bash
> $ python -m chatbot process --filepath <path_to_the_parsed_txt_file> --min_length 1 --max_length 10 --min_count 3 --save_path <filepath_to_save_processed_txt_file>
>```

### Create assets
> Create training, validation, and test sets by executing:
>```bash
> $ python -m chatbot create_assets --filepath <path_to_the_processed_txt_file> --val_size 0.2 --test_size 0.2 --random_seed 42 --assets_directory <local_directory_to_save_the_assets>
>```

### Training
> Define training parameters in the configuration file and from the project's root directory execute:
>```bash
> $ python -m chatbot train --config <path_to_training_yaml_file>
>```
> To resume training from a saved checkpoint execute:
>```bash
> $ python -m chatbot train --config ./configs/experiments.yaml --resume_from_checkpoint yes
>```
> The checkpoint path is defined in the configuration file.

### Evaluation
> Start chatting with the bot with:
>```bash
> $ python -m chatbot evaluate --config <path_to_evalation_yaml_file>
>```
> **â„¹ï¸ Important**
>
> Make sure that the initialization parameters for each of the seq2seq model components defined in
> the evaluation configuration file match with those in the training configuration file.

## Experiment Tracking ğŸ“‰
Not implemented yet.

## References ğŸ“š
1. [Effective Approaches to Attention-based Neural Machine Translation. Luong, M.-T., Pham, H., & Manning, C. D.(2015)](https://arxiv.org/abs/1508.04025)
2. [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer.](https://arxiv.org/abs/1506.03099)
3. [Perkins, Jacob. *Natural Language Processing*. O'Reilly Media, 2017.](https://www.oreilly.com/library/view/natural-language-processing/9781491978221/)
4. [Matthew Inkawhich, PyTorch Chatbot Tutorial. PyTorch Tutorials.](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)
