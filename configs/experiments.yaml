#----------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------
# - train_data_path         : The path to the txt file containing the training pairs
#                             in order to build the vocabulary.
# - val_data_path           : The path to the txt file containing the validation pairs.
# - checkpoints_dir         : The directory path to save checkpoints during training.
# - checkpoint_filename     : The name of the checkpoint file.
# - embeddings_path         : Path to pre-trained embeddings file, i.e. GLoVe embeddings.
# - batch_size              : How many samples per batch to load.
# - num_epochs              : An integer indicating how many epochs to train for.
# - ea_patience             : Number of epochs to wait before early stopping.
# - ea_delta                : Minimum change in monitored quantity to qualify as an
#                             improvement.
# - clip_factor             : Max norm value for gradient clipping.
# - enable_early_stop       : If ``True``, enables early stop functionality.
# - enable_lr_scheduler     : If ``True``, enables the learning schedulers for the encoder
#                             and decoder layers.
# - sampling_decay          : Integer >= 1. Smaller value results in a quicker drop of the
#                             sample probability per epoch.
# - sampling_method         : Samples the decoders' predictions. Available
#                             methods: ``greedy``, ``random``.
# - alignment_method        : Name of the alignment score method used in the attention layer.
#                             The available methods are: `concat`, `dot` and `general`.
# - temperature             : Float number > 0. Used to control the randomness of the decoder's
#                             predictions by scaling the logits before applying softmax.
# - embedding_dim           : Dimensions of token embeddings.
# - hidden_size             : Hidden size of `rnn`, `gru` or `lstm` model.
# - dropout                 : Dropout value to apply after the `rnn`, `gru or `lstm`
#                             layer.
# - num_layers              : Number of GRU layers in the encoder/decoder layer.
# - lr                      : The learning rate of the encoder/decoder optimizer.
#----------------------------------------------------------------------------------------
train_data_path: ./data/cornell-dialogs/assets/train_set.txt
val_data_path: ./data/cornell-dialogs/assets/val_set.txt
checkpoints_dir: ./checkpoints
checkpoint_filename: base_model.pth
hyperparameters:
    embeddings_path:
    general:
      batch_size: 64
      num_epochs: 50
      ea_patience: 6
      ea_delta: 0
      clip_factor: 50
      enable_early_stop: True
      enable_lr_scheduler: True
      sampling_decay: 16
      sampling_method: random
    embedding_init_params:
      embedding_dim: 50
      padding_idx: 0
    encoder_init_params:
      hidden_size: 50
      num_layers: 1
      dropout: 0.1
    decoder_init_params:
      alignment_method: dot
      hidden_size: 50
      num_layers: 1
      dropout: 0.1
      temperature: 1.0
    encoder_optimizer_init_params:
      lr: 0.0001
      weight_decay: 0.001
    decoder_optimizer_init_params:
      lr: 0.0001
      weight_decay: 0.001
    encoder_lr_scheduler_init_params:
      factor: 0.3
      patience: 1
    decoder_lr_scheduler_init_params:
      factor: 0.3
      patience: 1
